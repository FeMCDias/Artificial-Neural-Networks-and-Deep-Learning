{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a775da",
   "metadata": {},
   "source": [
    "\n",
    "# Variational Autoencoder (VAE) on Fashion‑MNIST — **PyTorch**\n",
    "\n",
    "This notebook reimplements the activity **using PyTorch**, and is **fully annotated** for clarity: data prep → model → training →\n",
    "evaluation → visualization → sampling, plus an **AE baseline** for extra credit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044a875",
   "metadata": {},
   "source": [
    "\n",
    "## Dependencies\n",
    "\n",
    "- `torch` (CPU is fine)\n",
    "- `pandas`, `numpy`, `matplotlib`\n",
    "\n",
    "> If you don't have PyTorch yet, install a CPU‑only wheel:\n",
    "> ```bash\n",
    "> pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a797ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def seed_everything(seed: int = 7):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(7)\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "ASSETS = ROOT / \"assets\"\n",
    "DATA = ROOT / \"data\"\n",
    "ASSETS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339fceb",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data Preparation (CSV → PyTorch DataLoader)\n",
    "\n",
    "We use the Kaggle‑style CSVs where the **first column is the label** and the remaining **784 columns** are pixel values.\n",
    "We normalize to **[0,1]**, split **90/10** train/validation, and create `DataLoader`s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ae7bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class FashionCSVDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        y = df.iloc[:, 0].to_numpy(dtype=np.int64)\n",
    "        X = df.iloc[:, 1:].to_numpy(dtype=np.float32) / 255.0\n",
    "        self.x = torch.from_numpy(X)   # (N, 784) in [0,1]\n",
    "        self.y = torch.from_numpy(y)   # (N,)\n",
    "    def __len__(self): return self.x.shape[0]\n",
    "    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
    "\n",
    "train_csv = DATA / \"fashion-mnist_train.csv\"\n",
    "test_csv  = DATA / \"fashion-mnist_test.csv\"\n",
    "\n",
    "full_train = FashionCSVDataset(train_csv)\n",
    "N = len(full_train)\n",
    "n_val = int(0.1 * N)\n",
    "n_train = N - n_val\n",
    "train_set, val_set = random_split(full_train, [n_train, n_val], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "# subsample for speed while prototyping (set to None to use all)\n",
    "SUB_TRAIN = 15000\n",
    "SUB_VAL   = 5000\n",
    "if SUB_TRAIN is not None and SUB_TRAIN < n_train:\n",
    "    train_set = torch.utils.data.Subset(train_set, range(SUB_TRAIN))\n",
    "if SUB_VAL is not None and SUB_VAL < n_val:\n",
    "    val_set = torch.utils.data.Subset(val_set, range(SUB_VAL))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "len(train_set), len(val_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ead295",
   "metadata": {},
   "source": [
    "\n",
    "## 2) VAE in PyTorch\n",
    "\n",
    "**Encoder**: `x → ReLU → (μ, logσ²)`  \n",
    "**Reparameterization**: `z = μ + σ·ε` with `ε ~ N(0, I)`  \n",
    "**Decoder**: `z → ReLU → logits(784)`  \n",
    "**Loss**: `ELBO = BCEWithLogits + KL`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee681af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=784, hidden=256, latent=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(inplace=True))\n",
    "        self.mu = nn.Linear(hidden, latent)\n",
    "        self.logvar = nn.Linear(hidden, latent)\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent=2, hidden=256, out_dim=784):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(latent, hidden), nn.ReLU(inplace=True), nn.Linear(hidden, out_dim))\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim=784, hidden=256, latent=2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_dim, hidden, latent)\n",
    "        self.decoder = Decoder(latent, hidden, in_dim)\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_logits = self.decoder(z)\n",
    "        return recon_logits, mu, logvar\n",
    "    def decode_from_mu(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        return self.decoder(mu), mu, logvar\n",
    "\n",
    "def elbo_loss(recon_logits, x, mu, logvar, reduction=\"mean\"):\n",
    "    bce = nn.functional.binary_cross_entropy_with_logits(recon_logits, x, reduction=\"sum\")\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    if reduction == \"mean\":\n",
    "        N = x.size(0)\n",
    "        return (bce + kl) / N, bce / N, kl / N\n",
    "    return bce + kl, bce, kl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6946c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Training (ELBO)\n",
    "\n",
    "We use Adam, track **train** and **validation** ELBO, and save curves to `assets/`.\n",
    "For validation, we decode deterministically with `z=μ` to stabilize the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0203d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 375.257 | val 312.153\n",
      "Epoch 02 | train 302.491 | val 292.237\n",
      "Epoch 03 | train 289.729 | val 284.550\n",
      "Epoch 04 | train 284.528 | val 280.986\n",
      "Epoch 05 | train 281.714 | val 279.163\n",
      "Epoch 06 | train 279.918 | val 277.751\n",
      "Epoch 07 | train 278.719 | val 276.370\n",
      "Epoch 08 | train 277.558 | val 275.306\n",
      "Epoch 09 | train 276.439 | val 274.252\n",
      "Epoch 10 | train 275.517 | val 273.484\n",
      "Epoch 11 | train 274.949 | val 273.200\n",
      "Epoch 12 | train 274.044 | val 272.145\n",
      "Epoch 13 | train 273.427 | val 271.287\n",
      "Epoch 14 | train 272.505 | val 270.443\n",
      "Epoch 15 | train 272.168 | val 269.932\n",
      "Epoch 16 | train 271.465 | val 270.047\n",
      "Epoch 17 | train 271.002 | val 269.171\n",
      "Epoch 18 | train 270.754 | val 268.431\n",
      "Epoch 19 | train 270.134 | val 268.426\n",
      "Epoch 20 | train 269.703 | val 267.981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_vae(model, train_loader, val_loader, device, epochs=5, lr=1e-3):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    tr_hist, va_hist = [], []\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); ep_losses = []\n",
    "        for xb, _ in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            recon_logits, mu, logvar = model(xb)\n",
    "            loss, bce, kl = elbo_loss(recon_logits, xb, mu, logvar, reduction='mean')\n",
    "            loss.backward(); opt.step()\n",
    "            ep_losses.append(loss.item())\n",
    "        tr_hist.append(float(np.mean(ep_losses)))\n",
    "        model.eval(); val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                recon_logits, mu, logvar = model.decode_from_mu(xb)\n",
    "                vloss, _, _ = elbo_loss(recon_logits, xb, mu, logvar, reduction='mean')\n",
    "                val_losses.append(vloss.item())\n",
    "        va_hist.append(float(np.mean(val_losses)))\n",
    "        print(f\"Epoch {ep:02d} | train {tr_hist[-1]:.3f} | val {va_hist[-1]:.3f}\")\n",
    "    # Curves\n",
    "    plt.figure(); plt.plot(tr_hist); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('VAE — Training Loss (ELBO approx.)')\n",
    "    plt.tight_layout(); plt.savefig(ASSETS / 'vae_train_loss.png'); plt.close()\n",
    "    plt.figure(); plt.plot(va_hist); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('VAE — Validation Loss (ELBO approx.)')\n",
    "    plt.tight_layout(); plt.savefig(ASSETS / 'vae_val_loss.png'); plt.close()\n",
    "    return tr_hist, va_hist\n",
    "\n",
    "vae = VAE(in_dim=784, hidden=256, latent=2)\n",
    "EPOCHS = 30\n",
    "train_hist, val_hist = train_vae(vae, train_loader, val_loader, device, epochs=EPOCHS, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c665604",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Evaluation & Visualization\n",
    "\n",
    "- **Reconstructions (val, z=μ)** → `assets/vae_recon_val.png`  \n",
    "- **Latent scatter (μ)** → `assets/vae_latent_scatter.png` (2‑D or PCA)  \n",
    "- **Prior samples** → `assets/vae_samples.png`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0af5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_image_grid(images, grid_shape, out_png, suptitle=None):\n",
    "    R, C = grid_shape\n",
    "    fig, axes = plt.subplots(R, C, figsize=(C*1.5, R*1.5))\n",
    "    axes = np.array(axes).reshape(R, C)\n",
    "    k = 0\n",
    "    for i in range(R):\n",
    "        for j in range(C):\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(images[k], cmap='gray', vmin=0, vmax=1)\n",
    "            ax.axis('off'); k += 1\n",
    "            if k >= len(images): break\n",
    "    if suptitle: fig.suptitle(suptitle, y=0.98)\n",
    "    plt.tight_layout(); plt.savefig(out_png, bbox_inches='tight'); plt.close()\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    xb, yb = next(iter(val_loader))\n",
    "    xb = xb.to(device)\n",
    "    recon_logits, mu_b, logvar_b = vae.decode_from_mu(xb)  # deterministic\n",
    "    recon = torch.sigmoid(recon_logits).cpu().numpy().reshape(-1, 28, 28)\n",
    "    orig  = xb.cpu().numpy().reshape(-1, 28, 28)\n",
    "    grid = np.vstack([orig[:12], recon[:12]])\n",
    "    save_image_grid(grid, (4,6), ASSETS / 'vae_recon_val.png', 'VAE — Originals (top) vs Reconstructions (bottom)')\n",
    "\n",
    "    # Latent scatter\n",
    "    all_mu, all_y = [], []\n",
    "    for xb2, yb2 in val_loader:\n",
    "        xb2 = xb2.to(device)\n",
    "        mu2, _ = vae.encoder(xb2)\n",
    "        all_mu.append(mu2.cpu().numpy()); all_y.append(yb2.numpy())\n",
    "    MU = np.concatenate(all_mu, axis=0); Y = np.concatenate(all_y, axis=0)\n",
    "    if MU.shape[1] == 2:\n",
    "        pts = MU\n",
    "    else:\n",
    "        Xc = MU - MU.mean(0, keepdims=True)\n",
    "        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "        pts = Xc @ Vt[:2].T\n",
    "    if len(pts) > 2000:\n",
    "        idx = np.random.default_rng(0).choice(len(pts), size=2000, replace=False)\n",
    "        pts, Y = pts[idx], Y[idx]\n",
    "    plt.figure(figsize=(5.2, 4.4))\n",
    "    plt.scatter(pts[:,0], pts[:,1], c=Y, s=8, alpha=0.7)\n",
    "    plt.xlabel('z1'); plt.ylabel('z2'); plt.title('VAE — Latent Space (μ) on Validation')\n",
    "    plt.tight_layout(); plt.savefig(ASSETS / 'vae_latent_scatter.png'); plt.close()\n",
    "\n",
    "    # Prior samples\n",
    "    Z = torch.randn(24, 2, device=device)\n",
    "    samples = torch.sigmoid(vae.decoder(Z)).cpu().numpy().reshape(-1, 28, 28)\n",
    "    save_image_grid(samples, (4,6), ASSETS / 'vae_samples.png', 'VAE — Samples from N(0, I)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff31159",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Extra — Autoencoder (AE) Baseline\n",
    "\n",
    "The AE reconstructs well but is **not** generative. We compare curves and reconstructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e2586dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE Epoch 01 | train 0.468 | val 0.369\n",
      "AE Epoch 02 | train 0.346 | val 0.330\n",
      "AE Epoch 03 | train 0.323 | val 0.317\n",
      "AE Epoch 04 | train 0.313 | val 0.311\n",
      "AE Epoch 05 | train 0.308 | val 0.305\n",
      "AE Epoch 06 | train 0.304 | val 0.301\n",
      "AE Epoch 07 | train 0.301 | val 0.299\n",
      "AE Epoch 08 | train 0.298 | val 0.296\n",
      "AE Epoch 09 | train 0.296 | val 0.297\n",
      "AE Epoch 10 | train 0.295 | val 0.293\n",
      "AE Epoch 11 | train 0.293 | val 0.292\n",
      "AE Epoch 12 | train 0.291 | val 0.290\n",
      "AE Epoch 13 | train 0.290 | val 0.289\n",
      "AE Epoch 14 | train 0.289 | val 0.289\n",
      "AE Epoch 15 | train 0.288 | val 0.287\n",
      "AE Epoch 16 | train 0.287 | val 0.286\n",
      "AE Epoch 17 | train 0.286 | val 0.286\n",
      "AE Epoch 18 | train 0.286 | val 0.285\n",
      "AE Epoch 19 | train 0.285 | val 0.284\n",
      "AE Epoch 20 | train 0.284 | val 0.283\n",
      "AE Epoch 21 | train 0.283 | val 0.283\n",
      "AE Epoch 22 | train 0.282 | val 0.282\n",
      "AE Epoch 23 | train 0.282 | val 0.282\n",
      "AE Epoch 24 | train 0.281 | val 0.281\n",
      "AE Epoch 25 | train 0.281 | val 0.281\n",
      "AE Epoch 26 | train 0.280 | val 0.280\n",
      "AE Epoch 27 | train 0.280 | val 0.280\n",
      "AE Epoch 28 | train 0.279 | val 0.279\n",
      "AE Epoch 29 | train 0.279 | val 0.279\n",
      "AE Epoch 30 | train 0.279 | val 0.279\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim=784, hidden=256, latent=32):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(inplace=True), nn.Linear(hidden, latent))\n",
    "        self.dec = nn.Sequential(nn.Linear(latent, hidden), nn.ReLU(inplace=True), nn.Linear(hidden, in_dim))\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x); return self.dec(z)\n",
    "\n",
    "def train_ae(model, train_loader, val_loader, device, epochs=3, lr=1e-3):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    tr, va = [], []\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); ep_losses = []\n",
    "        for xb, _ in train_loader:\n",
    "            xb = xb.to(device); opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb); loss = bce(logits, xb)\n",
    "            loss.backward(); opt.step(); ep_losses.append(loss.item())\n",
    "        tr.append(float(np.mean(ep_losses)))\n",
    "        model.eval(); val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in val_loader:\n",
    "                xb = xb.to(device); logits = model(xb); val_losses.append(bce(logits, xb).item())\n",
    "        va.append(float(np.mean(val_losses)))\n",
    "        print(f\"AE Epoch {ep:02d} | train {tr[-1]:.3f} | val {va[-1]:.3f}\")\n",
    "    plt.figure(); plt.plot(tr); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('AE — Training Recon Loss (BCE)')\n",
    "    plt.tight_layout(); plt.savefig(ASSETS / 'ae_train_loss.png'); plt.close()\n",
    "    plt.figure(); plt.plot(va); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('AE — Validation Recon Loss (BCE)')\n",
    "    plt.tight_layout(); plt.savefig(ASSETS / 'ae_val_loss.png'); plt.close()\n",
    "    return tr, va\n",
    "\n",
    "ae = AE(in_dim=784, hidden=256, latent=32)\n",
    "ae_tr, ae_va = train_ae(ae, train_loader, val_loader, device, epochs=30, lr=1e-3)\n",
    "\n",
    "# Recon grid\n",
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    xb, yb = next(iter(val_loader))\n",
    "    xr = torch.sigmoid(ae(xb.to(device))).cpu().numpy().reshape(-1, 28, 28)\n",
    "    orig = xb.cpu().numpy().reshape(-1, 28, 28)\n",
    "    grid = np.vstack([orig[:12], xr[:12]])\n",
    "    # naming consistent with README\n",
    "    plt.figure()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53408c9f",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Wrap‑up\n",
    "\n",
    "- ELBO = reconstruction (BCE) + KL to a unit Gaussian prior.  \n",
    "- VAE learns a structured latent space for **sampling**; AE is **not** generative.  \n",
    "- Increase epochs/hidden size for better quality; try different latent sizes; use PCA (or t‑SNE/UMAP) for visualization when `latent_dim > 3`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
